李航《统计学习方法》笔记总结：
	

char 01 概论
	统计学习方法 = 模型 + 策略 + 算法

		模型：  假设空间中的条件概率分布或决策函数 
		策略：
			1）损失函数和风险函数
			2）经验风险函数最小化与结构风险最小化
		算法：
			指学习模型的绝体计算方法。
				统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。
				统计学习问题归结为最优化问题。

	基于损失函数的模型的训练误差和模型的测试误差

	正则化

	交叉验证

	泛化能力

	生成模式：朴素贝叶斯法、隐马尔科夫链
		还原联合概率分布、学习收敛速度快、存在隐变量仍可以用。
	判别模式：K近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法、条件随机场。
		直接学习的是条件概率或决策函数，直接面对预测，学习的准确率更高，数据抽象、定义特征、简化学习问题。

	分类问题：
		准确率：预测的准不准：TP/(TP+FP),预测正确的正例占预测正例总数的比例。
				你预测的正例中，有多少是真的，是预测对的。
		召回率：预测的全不全：TP/(TP+FN),预测正确的正例占实际正例总数的比例。
				是不是所有的正例都被你预测到，并且正确了。


char 02 感知机
	模型：超平面 分割 正负例
		求得模型（超平面的参数w,b）,即求得模型。

	策略：
		线性可分数据集：
			存在 w·x+b=0 超平面使得两侧正例负例异号。
		为了找到这个超平面，确定要给学习策略：定义（经验损失函数）：并将损失函数极小化。

			误分类点到超平面总距离最小化:
				-->> 误分类数据特点：sum:-y[i]*（w·x[i]+b）> 0 				-->>损失函数定义：L(w,b) = -sum:-y[i]*（w·x[i]+b）

	算法：
		基于上面损失函数定义：
			对loss Function基于参数w,b分别求偏导，得到梯度：
			W偏导：-sum:y[i]·x[i]
			b偏导：-sum:y[i]

			初始化w,b值，针对误分类点i，重新调整w,b。
			w <-- w + step*y[i]·x[i]
			b <-- b + step*y[i]              step为大于零小于等于1的步长，学习率。
				不断迭代，直到所有点都满足正确分类条件：
					y[i]*（w·x[i]+b）> 0：即预测值和实际值同号。

char 03 K 近邻法
	KNN Kd算法

	KNN 
		模型：
			三要素：
				距离度量:欧式距离、曼哈顿距离等。
				K值选择：一般选择较小数值，通常采用交叉验证法来选取最优K值。
					近似误差：小k值，近似误差减小。
					估计误差：但是对近邻的实例点太敏感，如果是噪声点，则易出错。

				分类决策规则：多数表决等价于经验风险最小化。
		算法：
			S1:遍历所有点到待测点的距离。
			S2:排序。
			S3:选出距离待测点最近的K个点。
			S4:根据K个点中：多数表决的策略，预测待测点分类。

	Kd树实现KNN：
		二叉排序树：
			从根节点开始分割树，直到所有叶节点：
				选择一个维度的中位数，划分左右子树。
				递归：左右子树选择一个维度的中位数划分子树。
					。。。

char 04 朴素贝叶斯法

	条件概率，将后验概率最大的类作为数据集x的类输出。
	极大似然估计：
	求 使得满足给定 样本集X 的条件下，输出Y=c[k]的条件概率最大的c[k]的取值。
		y = arg maxP(Y=c[k])*multiply:P(X[j]=x[j] | Y=c[K])
		求使得后验概率y最大的c[k]的值：
		计算for i in K:Y=c[i]中,满足P(Y)最大的c[i]值:为输出结果。

	以上是贝叶斯估计：
		在统计频数上，有些量可能为0，此时会导致有些概率值为零，此时的独立事件概率乘积也为零。
		分类会出现偏差：为此：
			在随机变量的各个取值频数上加上一个正数a，当a = 1 是就称为：
				拉普拉斯平滑。
					防止其某类分量为零的情况，从而解决极大似然估计零偏差问题。


char 05 决策树：
	信息熵、信息增益。

		

