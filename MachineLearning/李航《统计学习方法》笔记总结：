李航《统计学习方法》笔记总结：
	

char 01 概论
	统计学习方法 = 模型 + 策略 + 算法

		模型：  假设空间中的条件概率分布或决策函数 
		策略：
			1）损失函数和风险函数
			2）经验风险函数最小化与结构风险最小化
		算法：
			指学习模型的绝体计算方法。
				统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。
				统计学习问题归结为最优化问题。

	基于损失函数的模型的训练误差和模型的测试误差

	正则化

	交叉验证

	泛化能力

	生成模式：朴素贝叶斯法、隐马尔科夫链
		还原联合概率分布、学习收敛速度快、存在隐变量仍可以用。
	判别模式：K近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法、条件随机场。
		直接学习的是条件概率或决策函数，直接面对预测，学习的准确率更高，数据抽象、定义特征、简化学习问题。

	分类问题：
		准确率：预测的准不准：TP/(TP+FP),预测正确的正例占预测正例总数的比例。
				你预测的正例中，有多少是真的，是预测对的。
		召回率：预测的全不全：TP/(TP+FN),预测正确的正例占实际正例总数的比例。
				是不是所有的正例都被你预测到，并且正确了。


char 02 感知机
	模型：超平面 分割 正负例
		求得模型（超平面的参数w,b）,即求得模型。

	策略：
		线性可分数据集：
			存在 w·x+b=0 超平面使得两侧正例负例异号。
		为了找到这个超平面，确定要给学习策略：定义（经验损失函数）：并将损失函数极小化。

			误分类点到超平面总距离最小化:
				-->> 误分类数据特点：sum:-y[i]*（w·x[i]+b）> 0 				-->>损失函数定义：L(w,b) = -sum:-y[i]*（w·x[i]+b）

	算法：
		基于上面损失函数定义：
			对loss Function基于参数w,b分别求偏导，得到梯度：
			W偏导：-sum:y[i]·x[i]
			b偏导：-sum:y[i]

			初始化w,b值，针对误分类点i，重新调整w,b。
			w <-- w + step*y[i]·x[i]
			b <-- b + step*y[i]              step为大于零小于等于1的步长，学习率。
				不断迭代，直到所有点都满足正确分类条件：
					y[i]*（w·x[i]+b）> 0：即预测值和实际值同号。

char 03 K 近邻法
	KNN Kd算法

	KNN 
		模型：
			三要素：
				距离度量:欧式距离、曼哈顿距离等。
				K值选择：一般选择较小数值，通常采用交叉验证法来选取最优K值。
					近似误差：小k值，近似误差减小。
					估计误差：但是对近邻的实例点太敏感，如果是噪声点，则易出错。

				分类决策规则：多数表决等价于经验风险最小化。
		算法：
			S1:遍历所有点到待测点的距离。
			S2:排序。
			S3:选出距离待测点最近的K个点。
			S4:根据K个点中：多数表决的策略，预测待测点分类。

	Kd树实现KNN：
		二叉排序树：
			从根节点开始分割树，直到所有叶节点：
				选择一个维度的中位数，划分左右子树。
				递归：左右子树选择一个维度的中位数划分子树。
					。。。

char 04 朴素贝叶斯法

	条件概率，将后验概率最大的类作为数据集x的类输出。
	极大似然估计：
	求 使得满足给定 样本集X 的条件下，输出Y=c[k]的条件概率最大的c[k]的取值。
		y = arg maxP(Y=c[k])*multiply:P(X[j]=x[j] | Y=c[K])
		求使得后验概率y最大的c[k]的值：
		计算for i in K:Y=c[i]中,满足P(Y)最大的c[i]值:为输出结果。

	以上是贝叶斯估计：
		在统计频数上，有些量可能为0，此时会导致有些概率值为零，此时的独立事件概率乘积也为零。
		分类会出现偏差：为此：
			在随机变量的各个取值频数上加上一个正数a，当a = 1 是就称为：
				拉普拉斯平滑。
					防止其某类分量为零的情况，从而解决极大似然估计零偏差问题。


char 05 决策树：
	信息熵、信息增益。

	熵H(Y)与条件熵H(Y|X)之差称为互信息。
		


	CART算法：分类与回归树：
		通俗讲：既可以用于分类，又可以用于回归。
		二叉树

		两种树的生成所用的策略不同：
			回归树用的是：平方误差最小化准则，
			分类树用的是：基尼系数最小化原则。

		回归树：


		分类树：
			基尼系数：
			

	注意区分：
		CART分类树策略选择是基尼指数最小的特征，Gini指数与熵类似：基尼指数值越大，样本集合的不确定性也就越大。
			但是要注意一点：
			在ID3算法中，    我们选择的是 信息熵增益最大 的特征及其对应的切分点作为最优特征与最有切分点，
			而在CART分类树中，我们用的是 基尼指数最小 的特征及其对应的切分点作为最优特征与最有切分点。
				原因在于：选择信息增益熵最大的特征，其实就是选择了信息熵最小的特征。
		重点：：：：
				因为原始信息信息熵很大，作为被减数，在已知某些条件下，信息的不确定性就被确定了部分，我们算的是经过某个特征条件假设后的条件概率下的信息熵，所以基于当前特征的提示下的信息熵越小，信息熵的增益的就越大。所以我们选择的还是信息熵最小的分类，信息熵最小也就意味着，确定性越强，对分类的指导意义也就越大。

		重点：：：：


	决策树主要侧重对于分支结点的选取，也即特征的选取：关键是其准则策略：
		1）信息增益：（ID3）
		2）信息增益比：(C4.5)
		3）基尼系数：（CART）
		不断递归，不断选取局部最优的特征。


		过拟合-->>剪枝



Char 06 逻辑斯蒂回归和最大熵模型


	条件熵：





Char 07 支持向量机

	训练数据  	线性可分： 		硬间隔最大化			线性可分支持向量机
	训练数据		近似线性可分：	软间隔最大化			线性支持向量机
	训练数据		线性不可分：		核函数、软间隔最大化	非线性支持向量机


	核函数：
		表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。
		通过使用核函数可以学习非线性支持向量机，等价于隐式地在高位的特征空间中学习线性支持向量机。

	训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。
		感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。（采用不同的初值或选取不同的误分类点，解不同）
		线性可分支持向量机利用 间隔最大化 求最优分离超平面，解是唯一的。


	函数间隔和几何间隔：
		函数间隔：一个点距离分离超平面的远近可以表示分类预测的确信程度。y·(w·x + b)来表示分类的正确性和确信度。
			原因1：|w·x + b|能够相对的表示点x距离超平面的远近。也就说确信程度。
			原因2：w·x + b的符号与类标记y的符号是否一致能够表示分类是否正确。
				所以：y·(w·x + b)来表示分类的正确性和确信度。
			
			选取样本中函数间隔 y·(w·x + b) 最小值。

		几何间隔：仅用函数间隔不足以确定超平面。对分类超平面的法向量w加约束：规范化：|w|=1。模为1。
			几何间隔：y·（w/|w|·x + b/|w|）


		函数间隔和几何间隔的关系：
			区别：就在于分母|w|上，几何间隔比函数间隔做了一个规范化约束。
			所以几何间隔才是我们需要的。	
		
		一般样本点到超平面的几何间隔为带符号的距离，只有当样本点被正确分类时才是真正的距离。


	支持向量机学习的基本思想：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。

		几何间隔最大的直观解释：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。



	本质由来：
		支持向量和间隔边界：
			支持向量：训练数据集中的样本点中与分离超平面距离 最近的样本点的实例 称为支持向量。
				支持向量是约束条件等号成立的点：y·(w·x + b)-1 = 0
					正例：w·x + b = 1
					负例：w·x + b = -1
			间隔边界：分离超平面与间隔边界平行且位于他们中央，边界的宽度：间隔。

			在决定分离超平面时只有 支持向量 起作用。而其他点并不起作用。如果移动支持向量将改变求的解。
			正是由于 支持向量 在确定分离超平面中起着决定性作用，所以将这种分类模型称为 支持向量机。
			支持向量类似于决策树中的最优特征，只要找到这个点，大于或者小于某个临界的条件就可以用来分类。





	线性支持向量机和软间隔最大化
		线性不可分的引入每一个样本点的松弛变量，考虑松弛变量之后，引入惩罚参数。这时得到的目标函数包含惩罚参数的软间隔最大化。
		下凸二次规划问题极值问题。

		线性支持向量机的解w唯一单b不一定唯一。




	非线性支持向量机与核函数

		核函数：通过一个函数完成坐标变换，使得原坐标系下线性不可分的数据集映射到高维度，使之成为线性可分数据集。
				经过映射函数f将原来的输入空间变换到一个新的特征空间，将输入空间中的内积x·y变成特征空间中的f(x)·f(y)。



	序列最小最优化算法（SMO）：
		将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。
			如果满足KKT条件：最优化的解就得到了。
			如果不满足：选择两个变量，其他变量固定：针对这两个变量构建一个二次规划问题：子问题。
				子问题两个变量：
					一个严重违反KKT条件，另一个由约束条件自动确定。
				外层循环确定第一个变量：违反KKT条件最严重的样本点。
				内层循环寻找第二个变量：能使得第二个变量变化足够大。




Char 08 提升方法

	提升方法：
		在分类问题中：通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

	思想：将多个专家的判断进行适当的综合所得出的判断。

	提升方法：从弱学习算法出发，反复学习，得到一系列弱分类器（基本分类器），然后组合这些弱分类器，构成一个强分类器。

	提升方法的两个问题：
		1、每一轮如何改变训练数据的权值或概率分布？
			AdaBoost做法：
				提高前一轮被弱分类器错误分类样本的权值，降低被正确分类样本的权值。
				这样：没有得到正确分类的数据由于其权值的加大而受到后一轮的弱分类器更大的关注。
		2、如何将弱分类器组合成一个强分类器？
			AdaBoost采取加权多数表决的方法。
				加大分类器误差率小的弱分类器的权值，使其在表决中起较大的作用；
				减小分类器误差率大的弱分类器的权值，使其在表决中起较小的作用。


	不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用。 

	AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。

	加法模型：基函数的线性组合。

	前向分步算法：
		因为学习的是加法模型，从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数。简化优化的复杂度。

	提升树：
		以分类树或回归树为基本分类器的提升方法。

		以决策树为基函数的提升方法为提升树。
			二叉分类树
			二叉回归树

		决策树桩：


	提升方法是弱学习算法提升为强学习算法的统计学习方法。


Char 09 EM算法及其推广

	EM是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计或极大后验概率估计。

	EM算法每次迭代由两步组成：
		E步：求期望
		M步：求极大
			所以 这一算法称为期望极大算法。

	EM迭代算法：与初值的选择有关，选择不同的初值可能得到不同的参数估计值。

	用Y表示观测随机变量的数据，Z表示隐随机变量的数据。
		Y和Z连载一起称为完全数据。
			观测数据Y又称为不完全数据。


	EM算法并不保证找到全局最优值。


	 EM算法的应用：k-means算法是EM算法思想的体现，E步骤为聚类过程，M步骤为更新类簇中心。GMM（高斯混合模型）也是EM算法的一个应用。



Char 10 隐马尔可夫模型

	标注问题。生成模型。

	隐马尔可夫模型是关于 时序 的概率模型，
		描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测二产生的观测随机序列的过程。

	隐藏的马尔科夫链随机生成的状态的序列：状态序列。
	每个状态生成一个观测，而由此产生的观测的随机序列，称为：观测序列。
	序列的每一个位置又可以看做一个：时刻。
	
