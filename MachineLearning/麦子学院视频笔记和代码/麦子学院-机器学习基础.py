麦子学院-深度学习

基础：机器学习
	就业方向：市场需求：机器学习，数据挖掘和统计分析人才


3.2决策树应用

		sklearn对输入数据有格式化的要求，只能处理数值型数据。所以需要数据转化：

		特征取值情况：
			      age：	youth、middle_aged、senior
	 		   income:	high、medium、low
			  student:	no、yes
		credit_rating:	fair、excellent
		Class_buys_co: no、yes

		age,对于每个个体样本：一维转化为三维：基因编码：
							age                          income        student
				youth   middle_aged     senior    high  medium  low   no     yes 
	样本1：		1        0                 0        0      1      0    1      0
	样本2：		0		 1				   0		1      0      0    0      1
	  。
	  。
	  。


6.1：神经网络
	
	https://www.cnblogs.com/subconscious/p/5058741.html


	神神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。
		初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，
		既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。




	理论证明，两层神经网络可以无限逼近任意连续函数。
		两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，
			本质上就是对向量的坐标空间进行一个变换。




	隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。
		而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。
		但是，节点数设置的多少，却会影响到整个模型的效果。
		如何决定这个自由层的节点数呢？
	
	目前业界没有完善的理论来指导这个决策。	
		一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。
			这种方法又叫做Grid Search（网格搜索）。



	尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。
		同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。


	“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，
		之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。
			这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“深度学习”。



	更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。
		而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，
			更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。












