《深度学习》花书

第5章 机器学习基础

5.1 学习算法
	机器学习算法是一种能够从数据中学习的算法。
		所谓“学习”：“对于某类任务T和性能度量P,一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”
			通过经验以提高计算机程序在某些任务性能的算法。
	5.1.1 任务T
		通常机器学习任务定义为机器学习系统应该 '如何处理样本（example）'。
			样本：某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。

5.2 容量、过拟合和欠拟合
	泛化：
		机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前为观测到的输入上表现良好的能力被称为泛化。
			就是说训练出来的模型，在训练集以外的数据上表现良好的程度。

	训练集和测试集数据通过数据集上被称为 数据生成过程（data generating Process）的概率分布式生成。
		独立同分布假设：每个数据集中的样本都是彼此相互独立的，并且训练集和测试集是同分布的，采样自相同的分布。这个假设使我们能够在单个样本的概率分布上描述数据生成过程。相同的分布可以用来生成每一个训练样本和每一个测试样本。（这个共享的潜在分布称为数据生成分布）===>>>随机模型训练误差的期望和该模型测试误差的期望是一样的。（都使用了相同的数据集生成过程。）

	过拟合和欠拟合：
		决定机器学习算法效果的因素：
			（1）降低训练误差
			（2）缩小训练误差和测试误差的差距

		测试误差 与 训练误差：
			欠拟合： 模型不能再训练集上获得足够低的训练误差。
			过拟合： 训练误差和测试误差之间的差距太大。

	通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。
		模型的容量：指其拟合各种函数的能力。
			容量低的模型可能很难拟合训练集。
			容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。

		一种控制训练算法容量的方法是选择假设空间：即学习算法可以选择为解决方案的函数集。
		
		当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，算法效果通常会最佳。容量不足的模型不能解决复杂任务。容量高的模型能够解决复杂的任务，但是档期容量高于任务所需时，有可能会过拟合。

	奥卡姆剃刀原则：Occam's razor。

	5.2.1 没有免费午餐定理：
		博弈论-零和博弈-
		学习理论表明机器学习算法能够在有限个训练集样本中很好的泛化：
			（所以说，学习一个东西，不需要面面俱到，把握主线就够了。）
			（不需要全学，但是碰到了能够解决。主线主干必须要有。）
			（水至清则无鱼。）
		没有一个机器学习算法总是比其他的要好。

