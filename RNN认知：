RNN：


	用于处理序列数据的神经网络。
		序列数据：
			在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。
			序列数据有一个特点：
				后面的数据跟前面的数据有关系。

	RNN:在层之间的神经元之间也建立的权连接。
	标准RNN:
		1、权值共享，图中的W全是相同的，U和V也一样。 
		2、每一个输入值都只与它本身的那条路线建立权连接，不会和别的神经元连接。
		
		采用BPTT算法训练RNN的方法。本质还是BP算法，本质也还是梯度下降法。






2、https://blog.csdn.net/zhaojc1995/article/details/80572098
	1）本文中解释了  梯度爆炸 和 梯度消失  的问题：累乘导致导数累乘，多用tanh函数作为激活函数。
		而sigmoid导数在（0，0.25],所以越乘越小。
		tanh函数导数范围：（0,1],要比sigmoid函数效果好点。
			tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。

			还有一个原因是sigmoid函数还有一个缺点，Sigmoid函数输出不是零中心对称。sigmoid的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。

		在较为深层的神经网络中使用sigmoid函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，
		那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。





	2）解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有：：

		1、选取更好的激活函数 
		2、改变传播结构

			关于第一点，一般选用ReLU函数作为激活函数，
				ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。
				还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习旅）也可以有效避免这个问题的发生。	
			关于第二点，LSTM结构可以解决这个问题。
				LSTM（long short-term memory）
					长短期记忆网络是RNN的一种变体。
						RNN由于梯度消失的原因只能有短期记忆，LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题。
						RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。 

总结一下，sigmoid函数的缺点： 
	1、导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。tanh函数导数值范围更大，相对好一点。 
	2、sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。


3、LSTM
	LSTM 的关键就是细胞状态,细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。 
	LSTM 拥有三个门，来保护和控制细胞状态。
		LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。、
			门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。
			Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！

		sigmoid函数选择更新内容，tanh函数创建更新候选。







感悟：日后可用：
	如果把每个字符编码都作为输出训练的模型会不会很好，在自然语言处理中，用每个字符编码做结果，然后训练模型，看看模型如何。此法后期可以尝试。

感悟二：球形网络。现在已知的网络都是二维的，如果有三维的立体或者更高维度的，那么可以解决一下 长期依赖 问题。


