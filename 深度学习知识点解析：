深度学习知识点解析：	


	通过使用Backpropagation可以更快的完成模型的训练。

	
	如何让神经网络训练更加迅速：
		优化：cost_function
		优化初始化：




	优化cost_function:
		Cross-Entropy Cost：


交叉熵损失函数问题：
	观察当只有一个神经元的时候的情况：
		原始cost_function和一个神经元的cost_function
		发现分别对偏差b和系数w求导之后。都是a*sigmoid(z)`的情况：
			观察：sigmoid函数发现：
				当神经元输出接近于1时，曲线很平缓。所以sigmoid的导数很小，学习很慢。
			如何增快学习？

		重新定义cost_function:C= -(1/n)sum[ylna+(1-y)ln(1-a)]

		对w求偏导：激活函数减去y
			好处：错误越大时，更新多，学得快，误差小时，学习慢。


		总结：
			cross-entropy cost几乎总是比二次cost函数好。
			如果神经元的方程是线性的，用二次cost函数（不会有学习慢的问题。）





其他的激活函数替代：

	softmax 
		激活函数：
			如果用sigmoid的话，最后不能保证默认输出总和为1.不能轻易的描述为概率分布。
			如果用softmax的话，就可以表示成概率的形式。

				而且：学习快慢的问题：取决于每一次更新算出来损失函数关于w和b的偏导数的大小。

			优势：	
				1、计算与标注样本的差距：取log里面的值就是这组数据正确分类的Softmax值，它占的比重越大，这个样本的Loss也就越小，这种定义符合我们的要求
				2、计算上非常非常的方便：通过梯度下降，每次优化一个step大小的梯度，只要将算出来的概率的向量对应的真正结果的那一维减1，然后再根据这个进行back propagation就可以了

	Tanh unit 
		f(x)= tanh(x)

	Rectified Linear unit (ReLU)
		f(x)=log(1+exp(x))		也称：softplus function

	Max(0,x+N(0,1))

	sigmoid = 1/(1+exp(-x))

		Sigmoid和ReL方程的主要区别：
			Sigmoid函数值：[0,1],
			ReL函数值：[0,无穷大]，
				所以：
					sigmoid函数方面来描述概率，
					而ReL适合用来描述实数。

			sigmoid函数的gradient随着x增大或减小而消失。
			ReL函数不会：
				gradient=0(if x<0)
				gradient=1(if x>0)
					所以：Rectified Linear Unit 在神经网络中的优势：
						不会产生vanishing gradient的问题。因为导数永远为1。


		所以在神经网络中，用ReL来代替sigmoid。


overfitting:
	训练集表现好，测试集表现非常差：
		如何判断是否发生了overfitting?训练集，验证集，测试集。 通过验证集，就是用来检测overfitting的。
			对于每一个epoch,在validation_data上面计算分类准确率。一旦accuracy在validation_data上面充分了，就停止训练。

	减小过拟合的办法一：
		通过修改训练集的大小就可以观察准确率的变化，最后也能说明：
			增大训练集可以帮助减少overfitting。

	减小过拟合的办法二：
		减小神经网络的规模，虽然：更深层更大的网络更有潜在的更强的学习能力。
		
	减小过拟合的办法三：
		regulation:正则化：
			（L1:L2两种正则化）
		加入regularization，不仅减小了overfitting,还避免了陷入局部最小点，更容易重现实验结果，提高了准确率。

		关于L1和L2更新其他资料，补充完善。

		都是减小权重：方法不同：
			L1减少一个常量，求导之后，正数为+1，负数为-1.（|w|w的绝对值导数)
			L2减少全中的一个固定比例。

			如果权重本身很大，L1减少的比L2少很多。
			如果权重本身很小，L1减少的更多。


	减小过拟合的办法四：
		Dropout
			也是为了为了减少overFitting
				和L1,L2 regularization不同,不是针对cost函数增加一项，而是对神经网络本身的结构做改变。


			每次用一半神经网络来训练，相当于多个神经网络组合起来，集群思维：
				S1：删除掉隐藏层随机选取的一半神经元。
				S2：在这个更改过的神经网络上正向和反向更新，利用一个mini-batch。
				S3：恢复之前删除掉的神经元，重新随机选择一半神经元删除，正向，反向，更新w,b。
				S4:重复上述过程。
				S5:最后，学习出来的神经网络的每个神经元都是在只有一半神经元的基础上学习的，当所有神经元被恢复后，为了补偿，我们把隐藏层的所有权重减半。


	减小过拟合的办法五：
		人工扩大训练集：
			主动产生更多的数据：
				旋转不同的角度。增加更多训练数据。





初始化权重问题：！！！！！！！！！！！！！！！！！！！！
	之前一般都是随机，随机从正态分布中产生：（均值为0，方差为1）

	这种初始化方法：缺陷在于：
		比如：1000个输入神经元，一半为1，一半为0，此时均值为0.5，方差为sqrt(501)=22.4

		是一个非常扁平化的正态分布。z的很多取值都远远大于1或者远远小于-1，这样的数据分布，在sigmoid函数中可以发现：
			1、sigmoid曲线在大于+1/-1时，曲线很平滑，此时更新的学习率就很慢，权重量更新小。


	这种初始化权重，过早的使隐藏层饱和了，跟之前我们说的输出层饱和问题相似，对于输出层，我们用改进的cost函数，比如：cross-entropy,但是对于隐藏层，我们无法通过cost函数来改进：
		如何更好的初始化权重。

		修改初始权重的分布：
			正态分布：
				均值为：		0
				标准差为：	1/sqrt(n_in)	n_in输入层个数开方。！！！！！！！！！！！！！！！！

			此时的z，绝大部分都在-1和1之间。神经元没有饱和，学习过程不会被减慢。






神经网络中的两种参数：
	目标参数：w,b

	超参：如何调整设置：

		学习率n 与 惩罚系数lmbda



神经网络中可变化调整的因素：！！！！！！！！

	1、神经网络的结构：层数、每层神经元的个数多少
	2、初始化w和b的方法
	3、cost函数
	4、Regularization
	5、Sigmoid输出还是Softmax
	6、使用Dropout
	7、训练集的大小
	8、mini-batch size
	9、学习率
	10：Regularization parameter:lambd:惩罚系数。

	总体策略：
		从简单的出发：
			先简化为使用0,1两类图。减少80%的数据量，用两层神经网络。
			获取更快的反馈，之前是每个epoch来检测准确率，可以替换为每1000个。
			或者减少validation set 的量，比如用100替代10000.

		grid-serach方法，可以在参数之间相互影响的条件下，将所有情况组合以网格的形式分别尝试。

		激活函数：sigmoid形式以外，还有tanh形式，还有 rectified linear神经元。
		优势：增加权重不会引起饱和，但加权的输入如果是负数，gradient就为0。
		具体哪种好，实践为主。



如果深度加深之后：深度学习。

	更深的好处：
		比如图像：可以从像素级，一级级的学习到更加抽象的高层。
			第一次边缘特征，第二层学习基本形状，第三层学到物体概念等。

		可以学习更加细致，每一层各种收获。

	如何训练深度神经网络：
		难点：神经网络的不同层学习的速率显著不同。
		接近输出层学习速率比较合适时，前面的层学习太慢，有时被困住。


深度神经网络中的难点1：
	神经网络的不稳定性：
	消失的gradient问题：vanishing gradient problem.

		在加入一层的神经网络中，准确率反而降低了。
		学习率：第一个隐藏层要比第四个隐藏层慢100倍。
		这种现象叫：vanishing gradient problem

		另外一种情况：内层的梯度比外层大很多。叫exploding gradient problem。

		所以神经网络算法用gradient之类的算法学习存在不稳定性。

		造成vanishing gradient problem 的原因：
			1、最终结果对偏差b1求导，观察，复合函数，连续求导，
			2、sigmoid函数求导发现，最大在零时，最大为1/4。
			3、而我们初始化参数时，往往N（0,1）分布。数值较小。所以如果连续相乘往往反而很小。
				所以连续求导，只会导致连续乘以更多的1/4。层数越多，最前面的导数越小，所以学习率越慢。
				也就说越往外，学习率越高，越往后推，学习率越低。越靠近输出层，学习率越大。越靠近输入层，学习率越小。

				解决方案 预想1：
					1、初始化比较打的权重：比如w1=w2=W3=W4=W5=100
					2、初始化bias使偏导数为零：调节bisa让z=wx+b=0

					b1= -100*a0
					z1 = 100*a0+ -100*a0 = 0
					此时导数可以取到最大，为1/4。这样此时每层是前一层的25倍，又出现了exploding的问题。

				从根本上讲：不是vanishing或者exploding的问题。而是后面层的梯度是前面层的累积的乘积。
				所以神经网络非常不稳定，唯一可能的情况是以上的连续乘积刚好平衡大约等于1，但是这种几率非常小。

				所以：这是一个不稳定的梯度问题，通常有多层后，每层网络都以非常不同的速率学习。

				总体：vanishing problem具有普遍性：		
					如果想要客服vanishing problem，需要保证每一层的累积w*(sigmoid)`>1,我们



深度神经网络中的难点2：
	2010 Glorot and Bengio* : sigmoid函数造成输出层的activation大部分饱和0，并且建议了其他的activation函数。

	2013 Sutskever,Martens, Dahl and Hinton*:随机初始权重和偏向时，提出momentum-based stochastic gradient descent







weight decay是提高最终收敛的正确率的还是提高收敛速度的？同理，momentum呢？normalization呢？

一、weight decay（权值衰减）的使用既不是为了提高你所说的收敛精确度也不是为了提高收敛速度，其最终目的是  防止过拟合。
在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight d
ecay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。

二、momentum是梯度下降法中一种常用的加速技术。对于一般的SGD，其表达式为,沿负梯度方向下降。而带momentum项的SGD则写生如下形式：

其中即momentum系数，通俗的理解上面式子就是，如果上一次的momentum（即）与这一次的负梯度方向是相同的，那这次下降的幅度就会加大，所以这样做能够达到加速收敛的过程。

三、normalization。如果我没有理解错的话，题主的意思应该是batch normalization吧。batch normalization的是指在神经网络中激活函数的前面，将按照特征进行normalization，这样做的好处有三点：

1、提高梯度在网络中的流动。Normalization能够使特征全部缩放到[0,1]，这样在反向传播时候的梯度都是在1左右，避免了梯度消失现象。

2、提升学习速率。归一化后的数据能够快速的达到收敛。

3、减少模型训练对初始化的依赖





