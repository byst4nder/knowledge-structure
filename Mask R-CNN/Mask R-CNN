Mask R-CNN


----------------------------------------------------------------------------------------------
		一、实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN
				https://blog.csdn.net/jiongnima/article/details/79094159
----------------------------------------------------------------------------------------------
前言：
	何凯明：Mask R-CNN是2017年ICCV的best paper。
	传统单任务的网络结构已经逐渐不再引人瞩目，取而代之的是集成、复杂、一石多鸟的多任务网络模型：Mask R-CNN。

	1）目标检测：目标框（bounding box）
	2）目标分类：类别（class）
	3）像素级目标分割：前景，背景

	Mask R-CNN继承于Faster R-CNN（2016）,在Faster R-CNN上面加了一个Mask Prediction Branch (Mask预测分支)，改良了ROI Pooling,提出了ROI Align。

	在解析Mask R-CNN之前，先分析一下Faster R-CNN。Faster R-CNN继承于Fast R-CNN(2015)，Fast R-CNN继承于R-CNN(2014)。

	R-CNN ===> Fast R-CNN ===> Faster R-CNN ===> Mask R-CNN
	(2014)		(2015)			(2016)				(2017)


1、R-CNN:（2014年）
	Ross Girshick 采用卷积神经网络进行目标检测：
	
	综述：
		将模型输入图片===>提取2000个待检测区域===>串联方式通过卷积神经网络 提取特征===> 通过SVM分类，得到物体的类别===>
		===>通过bounding box regression调整目标包围框的大小。

	步骤：
		S1:提取待检测对象：或叫图像标注。
			模型输入为一张图片，然后在图片上提取2000多个待检测区域。
				selective search:
					通过传统图像处理方法将图像分成若干块，然后通过一个SVM将属于同一目标的若干块拿出来。
					核心是一个SVM。分类。
		
		S2:特征提取：
			借助当时2012年最新成果AlexNet，网络模型：通过使用图像分类数据集训练一个用于提取特征的网络。

		S3:比对目标：
			使用SVM,用目标的标签（类别）与包围框的大小进行训练。


	贡献：
		1）使用了卷积神经网络进行特征提取
		2）使用了bounding box regression 进行目标包围框的修正。

	问题：
		1）耗时的selective search,一帧图像花费2秒
		2）耗时的串行式CNN前向传播，对于每一个ROI，都需要经过AlexNet提取特征，为所有的ROI提取特征。
		3）*** 三个模块分别训练 ***，对存储空间的消耗很大。

2、Fast R-CNN（2015年）
	为解决如上问题：
		Ross在2015年，提出Fast R-CNN：

		===> 还是采用selective search 提取2000个候选框;
		===> 通过一个神经网络对全图进行特征提取;
		===> 使用ROI Pooling Layer在全图上摘取每一个ROI对应的特征;
		===> 再通过全连接层（FC Layer）进行分类和包围框的修正。

	贡献：
		1）取代R-CNN的串行特征提取方式，
		2）除了selective search ,其他部分都可以结合在一起训练。

	缺点：耗时的selective search 依旧存在。

3、Faster R-CNN（2016年）
	为了改良selective search的耗时问题：
		取代Selective Search,直接通过一个Region Proposal network(RPN)生成待检测区域，
		在生成ROI区域的时候，时间也就从2s缩减到10ms。

	过程：
		S1：===>>> 共享卷积层为全图提取特征feature maps
		S2：===>>> 将features maps送入到RPN,
		S3：===>>> RPN生成待检测框（指定ROI的位置），并对ROI的包围框进行第一次修正。
		S4：===>>> 进入到Fast R-CNN架构，
					ROI Pooling Layer根据RPN的输出在feature map上面选择每个ROI对应的特征，并将维度值置为定值。
		S5：===>>> 全连接层（FC Layer）对框进行分类，并且对目标包围框进行第二次修正。 

	S1,S2,S3替换掉Selective Search和Fast R-CNN中的特征提取。

	Faster R-CNN 真正实现了端到端的训练（end-to-end training）。

	要理解Mask R-CNN，只有先理解Faster R-CNN。

	Faster R-CNN架构：
		三大部分：
			1) 共享的卷积层-backbone
			2) 候选区域生成网络-RPN
			3) 对候选区域进行分类的网络-classifier
			其中：RPN和classifier部分均对目标框有修正。classifier完全继承Fast R-CNN结构。

		模块细节：
			RPN工作原理：
				RPN依靠一个在共享特征图 conv feature map 上滑动的窗口 sliding window. 

				为每个位置生成9种预先设置好长宽比与面积的目标框（anchor）,
					三种面积（128*128， 256*256， 512*512）
					三种长宽比（1：1， 1:2， 2:1）

				共享特征图feature maps大约有 40 * 60， RPN生成的初始anchor总数为20000个左右（40* 60 *9）。

				RPN目的：
					1）判断anchor到底是前景还是背景：判断这个anchor到底有没有覆盖目标。
						使用SoftmaxLoss直接训练，在训练的时候排除掉超越边界的anchor;
					2）为属于前景的anchor进行第一次坐标修正。
						采用SmoothL1Loss进行训练。

				RPN的实现：
					RPN本质：树状结构。
						树干为 3x3 的卷积层，树枝为两个1x1的卷积层。
							第一个1×1的卷积层解决了 ***前后景的输出，***
							第二个1×1的卷积层解决了 ***边框修正的输出。***

				RPN细节：
					对于RPN输出的特征图中的每一个点，
					第一个1×1的卷积层输出了18个值：前后景的输出：
						因为每一个点对应9个anchor,每个anchor有一个前景分数和一个后景分数（9×2）。
							前景后景区分：
								如果一个anchor与ground truth的IoU在0.7以上，这个anchor就算前景positive;
								如果这个anchor与ground truth的IoU在0.3以下，这个anchor就算背景(negative)。

							作者进行RPN网络训练的时候，没有使用IoU在0.3和0.7之间的anchor,
							在训练anchor属于前景与背景的时候，是在一张图中，随机抽取了128个前景anchor与128个背景anchor。

								补充：（IoU：Intersection over Union :IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。）
				
					第二个1×1的卷积层输出了36个值：边框修正。
						因为是每一个点对应9个anchor，每个anchor对应了4个修正坐标的值9×4=36。
						边框修正主要由4个值完成，tx,ty,th,tw。
							tx和ty：修正后的框在anchor的x和y方向上做出的平移。
							th和tw：修正前的框长宽各自放大一定的倍数。
						如何训练：
							采用SmothL1Loss进行训练。
							不是对于所有的anchor，都需要进行anchor包围框修正的参数训练，只是对positive的anchors有这一步。
							因此，在训练RPN的时候，只有对128个随机抽取的positive anchors有这一步训练。


			RoI Pooling:
				为何需要RoI Pooling?
					在Fast R-CNN中，特征被共享卷积层一次性提取。
					对于每个RoI而言，需要从 共享卷积层上 **摘取** 对应的特征,并且送入全连接层进行分类。
					RoI Pooling主要做了两件事：
						第一件是为每个RoI选取对应的特征，
						第二件是为满足全连接层的输入需求。
							将每个RoI对应的特征的维度转化为某个定值。
						
						对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小


			Fast R-CNN的分类器：
				提取的RoI具体是什么类别(人，车，马等等)，一共C+1类(包含一类背景)。


			RoI边框修正训练
				RoI边框修正和RPN中的anchor边框修正原理一样，同样也是SmoothL1 Loss。
				RoI边框修正也是对于非背景的RoI进行修正，对于类别标签为背景的RoI，则不进行RoI边框修正的参数训练。

			   在训练分类器和RoI边框修正时，步骤如下所示：

				   0) 首先通过共享卷积层backbone提取特征,shared conv feature maps (40×60)。

				   1) 首先通过RPN生成约20000个anchor(40×60×9)。

				   2) 对20000个anchor进行第一次边框修正，得到修订边框后的proposal。

				   3) 对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围。

				   4) 忽略掉长或者宽太小的proposal。

				   5) 将所有proposal按照前景分数从高到低排序，选取前12000个proposal。

				   6) 使用阈值为0.7的NMS算法排除掉重叠的proposal。

				   7) 首先通过RPN生成约20000个anchor(40×60×9)。

		总的来说，Faster R-CNN的loss分两大块，第一大块是训练RPN的loss(包含一个SoftmaxLoss和SmoothL1Loss)，第二大块是训练Fast R-CNN中分类器的loss(包含一个SoftmaxLoss和SmoothL1Loss)，

		Faster R-CNN的训练方式有三种，描述如下：	
		   1) RPN和Fast R-CNN交替训练，这种方式也是作者采用的方式。

		   2) 近似联合RPN和Fast R-CNN的训练，在训练时忽略掉了RoI边框修正的误差，只对anchor做了边框修订，"近似联合"。

		   3) 联合RPN和Fast R-CNN的训练。

		交替训练的方式，步骤如下：

		   1) 使用在ImageNet上预训练的模型初始化共享卷积层并训练RPN。

		   2) 使用上一步得到的RPN参数生成RoI proposal。
				再使用ImageNet上预训练的模型初始化共享卷积层，训练Fast R-CNN部分(分类器和RoI边框修订)。

		   3) 将训练后的共享卷积层参数固定，同时将Fast R-CNN的参数固定，训练RPN。
				(从这一步开始，共享卷积层的参数真正被两大块网络共享)

		   4) 同样将共享卷积层参数固定，并将RPN的参数固定，训练Fast R-CNN部分。


		Faster R-CNN的测试流程和训练流程挺相似，描述如下：

		   1) 首先通过RPN生成约20000个anchor(40×60×9)通过RPN。

		   2) 对20000个anchor进行第一次边框修正，得到修订边框后的proposal。

		   3) 对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围。

		   4) 忽略掉长或者宽太小的proposal。

		   5) 将所有proposal按照前景分数从高到低排序，选取前6000个proposal。

		   6) 使用阈值为0.7的NMS算法排除掉重叠的proposal。

		   7) 针对上一步剩下的proposal,选取前300个proposal进行分类和第二次边框修正。


4、Mask R-CNN (2017年)
	Mask R-CNN只是在Faster R-CNN上面加了一个Mask Prediction Branch (Mask 预测分支)，并且改良了ROI Pooling，提出了ROI Align。

	RoI Pooling的问题：

		问题1：从输入图上的RoI到特征图上的RoI feature，RoI Pooling是直接通过四舍五入取整得到的结果。
			直接用round取的值，RoI Pooling过后的得到的输出可能和原图像上的RoI对不上。
			RoI Pooling Layer的四舍五入取整操作，可能会导致其进行了偏移。

		问题2：在将每个RoI对应的特征转化为固定大小的维度时，又采用了取整操作。

			这种取整操作(在Mask R-CNN中被称为quantization)对RoI分类影响不大，
			可是对逐像素的预测目标是有害的，因为对每个RoI取得的特征并没有与RoI对齐。
			因此，Mask R-CNN对RoI Pooling做了改进并提出了RoI Align。

			   RoI Align的主要创新点：
				   针对问题1，不再进行取整操作。
				   针对问题2，使用双线性插值来更精确地找到每个块对应的特征。
			   总的来说，
				   RoI Align的作用主要就是剔除了RoI Pooling的取整操作，
				   并且使得为每个RoI取得的特征能够更好地对齐原图上的RoI区域。

		在Mask R-CNN中的RoI Align之后有一个"head"部分，主要作用是将RoI Align的输出维度扩大，这样在预测Mask时会更加精确。
		
		在Mask Branch的训练环节，没有采用FCN式的SoftmaxLoss，反而是输出了K个Mask预测图(为每一个类都输出一张)，
		并采用average binary cross-entropy loss训练。
		
		当然在训练Mask branch的时候，输出的K个特征图中，也只是对应ground truth类别的那一个特征图对Mask loss有贡献。
			Lbox和Lmask都是对positive RoI才会起作用的。


	Mask R-CNN和FCIS做个比较：
		相同点：均继承了Faster R-CNN的RPN部分。
		不同点：
			对于FCIS，预测mask和分类是共享的参数。
			而Mask R-CNN则是各玩各的，两个任务各自有各自的可训练参数。




补充一：				
--------------------------------------------------------------------------------------------
		二、深度学习中IU、IoU(Intersection over Union)的概念理解以及python程序实现
		https://blog.csdn.net/iamoldpan/article/details/78799857
		https://oldpan.me/archives/understand-coco-metric				
--------------------------------------------------------------------------------------------
	IoU(Intersection over Union)：一种测量在特定数据集中检测相应物体准确度的一个标准。
		只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。

		1）ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）；
		2）我们的算法得出的结果范围。
	这个标准用于测量真实和预测之间的相关度，相关度越高，该值越高。

	IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。
	一般来说，这个score ＞ 0.5 就可以被认为一个不错的结果了。






补充二：
----------------------------------------------------------------------------
			三、目标检测之Loss：FasterRCNN中的SmoothL1Loss
		https://blog.csdn.net/wfei101/article/details/79809332
----------------------------------------------------------------------------
多任务损失： 多任务！！！！
	Fast R-CNN网络有两个同级输出层（cls score和bbox_prdict层），都是全连接层，称为multi-task。






补充三：
-------------------------------------------------------------------------------------------
				四、NMS和soft-nms算法
		https://www.cnblogs.com/zf-blog/p/8532228.html
--------------------------------------------------------------------------------------------

NMS在物体检测中的应用：

	物体检测中应用NMS算法的主要目的是消除多余（交叉重复）的窗口，找到最佳物体检测位置。






补充四、
--------------------------------------------------------------------------------------------
		实例分割初探，Fully Convolutional Instance-aware Semantic Segmentation论文解读
				https://blog.csdn.net/jiongnima/article/details/78961147
--------------------------------------------------------------------------------------------
	本文重新建立一个文档。
	深度学习计算机视觉领域：
		图像分类(AlexNet, VGG16等等)
		图像分割(以FCN为代表的一众论文)
		目标检测(R-CNN，Fast R-CNN和Fatser R-CNN，以及后来的YOLO和SSD，目标检测领域已经实现多任务)。

	   1. 将物体从背景中分离(测试结果上只是没有画出目标框)，即 目标检测。

	   2. 对检测到的物体进行逐像素提取，即 图像分割。

	   3. 对检测到的物体进行类别划分，即 图像分类。

  实例分割是一个很综合的问题，融合了目标检测，图像分割与图像分类。






----------------------------------------------------------------------------
					论文阅读学习 - Mask R-CNN
		https://blog.csdn.net/zziahgf/article/details/78730859
----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------


----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------