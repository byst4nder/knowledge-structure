Mask R-CNN


----------------------------------------------------------------------------
一、实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN
https://blog.csdn.net/jiongnima/article/details/79094159
----------------------------------------------------------------------------
前言：
	何凯明：Mask R-CNN是2017年ICCV的best paper。
	传统单任务的网络结构已经逐渐不再引人瞩目，取而代之的是集成、复杂、一石多鸟的多任务网络模型：Mask R-CNN。

	1）目标检测：目标框（bounding box）
	2）目标分类：类别（class）
	3）像素级目标分割：前景，背景

	Mask R-CNN继承于Faster R-CNN（2016）,在Faster R-CNN上面加了一个Mask Prediction Branch (Mask预测分支)，改良了ROI Pooling,提出了ROI Align。

	在解析Mask R-CNN之前，先分析一下Faster R-CNN。Faster R-CNN继承于Fast R-CNN(2015)，Fast R-CNN继承于R-CNN(2014)。

	R-CNN ===> Fast R-CNN ===> Faster R-CNN ===> Mask R-CNN
	(2014)		(2015)			(2016)				(2017)


1、R-CNN:（2014年）
	Ross Girshick 采用卷积神经网络进行目标检测：
	
	综述：
		将模型输入图片===>提取2000个待检测区域===>串联方式通过卷积神经网络 提取特征===> 通过SVM分类，得到物体的类别===>
		===>通过bounding box regression调整目标包围框的大小。

	步骤：
		S1:提取待检测对象：或叫图像标注。
			模型输入为一张图片，然后在图片上提取2000多个待检测区域。
				selective search:
					通过传统图像处理方法将图像分成若干块，然后通过一个SVM将属于同一目标的若干块拿出来。
					核心是一个SVM。分类。
		
		S2:特征提取：
			借助当时2012年最新成果AlexNet，网络模型：通过使用图像分类数据集训练一个用于提取特征的网络。

		S3:比对目标：
			使用SVM,用目标的标签（类别）与包围框的大小进行训练。


	贡献：
		1）使用了卷积神经网络进行特征提取
		2）使用了bounding box regression 进行目标包围框的修正。

	问题：
		1）耗时的selective search,一帧图像花费2秒
		2）耗时的串行式CNN前向传播，对于每一个ROI，都需要经过AlexNet提取特征，为所有的ROI提取特征。
		3）*** 三个模块分别训练 ***，对存储空间的消耗很大。

2、Fast R-CNN（2015年）
	为解决如上问题：
		Ross在2015年，提出Fast R-CNN：

		===> 还是采用selective search 提取2000个候选框;
		===> 通过一个神经网络对全图进行特征提取;
		===> 使用ROI Pooling Layer在全图上摘取每一个ROI对应的特征;
		===> 再通过全连接层（FC Layer）进行分类和包围框的修正。

	贡献：
		1）取代R-CNN的串行特征提取方式，
		2）除了selective search ,其他部分都可以结合在一起训练。

	缺点：耗时的selective search 依旧存在。

3、Faster R-CNN（2016年）
	为了改良selective search的耗时问题：
		取代Selective Search,直接通过一个Region Proposal network(RPN)生成待检测区域，
		在生成ROI区域的时候，时间也就从2s缩减到10ms。

	过程：
		S1：===>>> 共享卷积层为全图提取特征feature maps
		S2：===>>> 将features maps送入到RPN,
		S3：===>>> RPN生成待检测框（指定ROI的位置），并对ROI的包围框进行第一次修正。
		S4：===>>> 进入到Fast R-CNN架构，
					ROI Pooling Layer根据RPN的输出在feature map上面选择每个ROI对应的特征，并将维度值置为定值。
		S5：===>>> 全连接层（FC Layer）对框进行分类，并且对目标包围框进行第二次修正。 

	S1,S2,S3替换掉Selective Search和Fast R-CNN中的特征提取。

	Faster R-CNN 真正实现了端到端的训练（end-to-end training）。

	要理解Mask R-CNN，只有先理解Faster R-CNN。

	Faster R-CNN架构：
		三大部分：
			1) 共享的卷积层-backbone
			2) 候选区域生成网络-RPN
			3) 对候选区域进行分类的网络-classifier
			其中：RPN和classifier部分均对目标框有修正。classifier完全继承Fast R-CNN结构。

		模块细节：
			RPN工作原理：
				RPN依靠一个在共享特征图 conv feature map 上滑动的窗口 sliding window. 

				为每个位置生成9种预先设置好长宽比与面积的目标框（anchor）,
					三种面积（128*128， 256*256， 512*512）
					三种长宽比（1：1， 1:2， 2:1）

				共享特征图feature maps大约有 40 * 60， RPN生成的初始anchor总数为20000个左右（40* 60 *9）。

				RPN目的：
					1）判断anchor到底是前景还是背景：判断这个anchor到底有没有覆盖目标。
						使用SoftmaxLoss直接训练，在训练的时候排除掉超越边界的anchor;
					2）为属于前景的anchor进行第一次坐标修正。
						采用SmoothL1Loss进行训练。

				RPN的实现：
					RPN本质：树状结构。
						树干为 3x3 的卷积层，树枝为两个1x1的卷积层。
							第一个1×1的卷积层解决了 ***前后景的输出，***
							第二个1×1的卷积层解决了 ***边框修正的输出。***

				RPN细节：
					对于RPN输出的特征图中的每一个点，
					第一个1×1的卷积层输出了18个值：前后景的输出：
						因为每一个点对应9个anchor,每个anchor有一个前景分数和一个后景分数（9×2）。
							前景后景区分：
								如果一个anchor与ground truth的IoU在0.7以上，这个anchor就算前景positive;
								如果这个anchor与ground truth的IoU在0.3以下，这个anchor就算背景(negative)。

							作者进行RPN网络训练的时候，没有使用IoU在0.3和0.7之间的anchor,
							在训练anchor属于前景与背景的时候，是在一张图中，随机抽取了128个前景anchor与128个背景anchor。

								补充：（IoU：Intersection over Union :IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。）
				
					第二个1×1的卷积层输出了36个值：边框修正。
						因为是每一个点对应9个anchor，每个anchor对应了4个修正坐标的值9×4=36。
						边框修正主要由4个值完成，tx,ty,th,tw。
							tx和ty：修正后的框在anchor的x和y方向上做出的平移。
							th和tw：修正前的框长宽各自放大一定的倍数。
						如何训练：
							采用SmothL1Loss进行训练。
							不是对于所有的anchor，都需要进行anchor包围框修正的参数训练，只是对positive的anchors有这一步。
							因此，在训练RPN的时候，只有对128个随机抽取的positive anchors有这一步训练。


			RoI Pooling:



				
----------------------------------------------------------------------------
二、深度学习中IU、IoU(Intersection over Union)的概念理解以及python程序实现
https://blog.csdn.net/iamoldpan/article/details/78799857
https://oldpan.me/archives/understand-coco-metric				
----------------------------------------------------------------------------
	IoU(Intersection over Union)：一种测量在特定数据集中检测相应物体准确度的一个标准。
		只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。

		1）ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）；
		2）我们的算法得出的结果范围。
	这个标准用于测量真实和预测之间的相关度，相关度越高，该值越高。

	IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。
	一般来说，这个score ＞ 0.5 就可以被认为一个不错的结果了。


----------------------------------------------------------------------------
论文阅读学习 - Mask R-CNN
https://blog.csdn.net/zziahgf/article/details/78730859
----------------------------------------------------------------------------
----------------------------------------------------------------------------

----------------------------------------------------------------------------


----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------

----------------------------------------------------------------------------