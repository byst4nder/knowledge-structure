计算机视觉

**********************************************************************
*** 															   ***
*** 						直捣黄龙								   ***
*** 															   ***
***																   *** 
***				越是不负责任，越是不计后果，越容易成功。				   ***
***																   ***
***																   ***
***																   ***
***  建议还是不要一开始就抓着一堆看着就头疼的理论读来读去,效率比较低。   ***
***  其实现在这一行，大多都是这样学比较好吧。还有，想到一定要做到！      ***
**********************************************************************




--------------------------------------------------------------------
				Computer Vision—计算机视觉 （一）
	https://blog.csdn.net/qq_35999634/article/details/81672009
--------------------------------------------------------------------
亮点：
	结构清晰：
	给出的建议非常宝贵！！！直捣黄龙！！！

一、传统CV

  	传统CV说白了就是特征+分类器。

	单张图片 ———>>> 大量图片 ———>>> 视频。

二、单张图片

  	依赖于图片的像素矩阵表示，有了矩阵，很多方法如二值化，阈值化，色彩均化，滤波（模糊/光滑），形态学开集和闭集，以及联通区域划分，图像金字塔等都可以做。这些处理方法在实际当中应用广泛，例如许多图片应用的滤镜/增强/变形效果,以及图片压缩。

三、大量图片

  	图片分类以及基于内容的检索等实际需求。
	传统机器视觉的方法或者说套路是，先针对问题和对特征的具体要求（例如希望特征具有旋转不变性等）设计一些特征抽取方法，有了特征之后，就能拿去喂给一些机器学习算法做分类等其他后续工作。

	那么特征提取就是重点，如颜色直方图，只是一个简单的统计描述，而其他常用的特征，如Harris角点，FAST角点，图像梯度以及HOG, LBP(local binary pattern), SIFT特征以及其变体SUFT和ORB, haar 等。
	经过精心设计，并且涉及一些诸如利用积分图像来优化计算的技巧，是传统计算机视觉的重要成果。

	有了特征之后，就能解决：边缘检测/轮廓提取，图像分割，图片分类，人脸识别，图片拼接(image stitching)，视觉测距（包括单目视觉测距(Monocular Visual Odometry)和立体视觉测距(Stereo Visual Odometry)）等经典问题。

四、视频

	视频无非就是连续（帧）的图片，图片处理技术就自然而然地扩展到了视频上来（一般的话，图片是一帧数据，视频处理就是在图片处理的   外面加一个大循环  ，不断的处理一帧又一帧的图片就行。
	当然图片或视频的获取方式有不少，USB、HDMI或者是直接从内存读取等等，不同方式下图片和视频读取的方式也有差别，但大同小异），但是视频具有动态特征也有其特殊的对待，如 均值漂移、 GMM（混合高斯模型） 等背景建模的方法，以及利用 光流法 等实现物体的跟踪。 
		（附：传统方法使用 ·滑动窗口· 的框架，把一张图分解成几百万个 不同位置 不同尺度 的 子窗口，针对每一个窗口使用 分类器 判断 是否包含 目标物体。
			传统方法针对不同的类别的物体，一般会设计不同的特征和分类算法，比如
				
				人脸检测的经典算法是Harr特征+Adaboosting分类器；
				行人检测的经典算法是HOG(histogram of gradients) + Support Vector Machine；
				一般性物体的检测的话是HOG的特征加上DPM(deformable part model)的算法)

				（都用HOG，所以要好好研究HOG）

	之后，随着人工智能技术的崛起，深度学习大潮席卷了包括计算机视觉和自然语言处理在内的诸多领域，许多在之前无法有效解决的问题，如图像语义分析、图片/视频内容描述、图片/视频问答等开始得到解决。许多新的问题，如图片生成（GAN）、图片风格迁移、图像预测、图像|视频搜索、OCR等开始出现，并在深度学习的火炬下显示出无限可能，而传统的方法，那些人工设计特征的时代，似乎正在渐行渐远。

五、基于机器学习|深度学习的CV

	这是CV领域以后的发展方向，包括但不限于：
		图像检测、图像分类、图像分割、图像跟踪、视频语义分析、人脸识别与分析、车辆与人员的检测识别与跟踪、图像/视频搜索、页面分析与自动合成、OCR等算法与系统研发领域。

	最主要的原因是，精度，深度学习可以做到传统方法无法企及的精度。

	另外，深度学习其他原因|优点：通用性强，特征迁移能力强，工程框架统一等。

	深度学习技术框架是一颗树形结构：

  		训练平台是树根，如tensorflow、theano等；

  		模型（网络结构）是树干，是深度学习的重点。
			典型成果有AlexNet、VGGNet、GoogleNet、ResNet等。
				
				学术界—怎么提高模型精度，
				工业界—还要考虑怎么把模型做得更快，更小。

 		（核心）任务|方向是树枝，检测、识别、分割、特征点定位、序列学习等5大任务。
			任何计算机视觉的具体应用都可以由这五个任务组合而成，如人脸识别，涉及到人脸检测、特征点定位，特征提取&验证。这就包含了检测、识别、特征点定位三个部分。

	方向-网络对应（常用）
		
		1)图像识别：
			Alexnet、VGGnet,GoogleNet,ResNet, RetinaNet

		2)目标检测：
			Fast-rcnn, faster-rcnn,Yolo, Retina-Net

		3)图像分割：
			FCN, Mask-RCNN

		4)目标跟踪：
			GOTURN, ECO

		5)图像生成：
			GAN, WGAN

		6)光流：
			FlowNet

		7)视频分割：
			SegNet

	建议：
		CV+深度学习的内容确实太多（废话，钱给的多），学习、入门感觉难度不小，但是还是有方法的。

		比如：实践+理论，
			具体就是大致了解整个框架情况后，直接找个具体实例，像人脸识别、行人|车辆检测都不错，仅用传统方法和传统+深度学习的都需要学习。

			这些例子里就是在用那些看上去很牛X的理论，各种图像预处理、特征提取|降维、训练|分类器、后处理，各种像检测|识别|分割|特征点定位等这些方法、数据集、模型、框架平台等等，通过1-2个这样具体的例子，大致弄懂功能——复现——读源码|理解原理，里面程序有搞不懂的就去查，直到搞懂。然后基本就入门有感觉了，卯足劲儿再去深入学习，后面内容还多着呢。

			（建议还是不要一开始就抓着一堆看着就头疼的理论读来读去，效率比较低。其实现在这一行，大多都是这样学比较好吧。还有，想到一定要做到！）



个人感悟：
	1、单张图像：通过传统的OpenCV处理方式，基于图片的像素矩阵来处理。
	2、视觉入门：从OpenCV传统手段入手，逐步引入01特征提取，02图像分类，03图像分割。。。。考虑新建01特征提取方法。
	3、整理hog，haar等方法。以及常见特征提取常见应用。



--------------------------------------------------------------------
			什么深度学习几乎成了计算机视觉研究的标配？
			https://zhuanlan.zhihu.com/p/21533690
--------------------------------------------------------------------
高屋建瓴！

不要过多沉溺在传统OpenCV上，OpenCV就几个算法（Harr,hog,DPM）值得一看，尽快过一下，然后转入CV三个部分上来。
分类顺序：AlexNet、VGGNet、GoogleNet、ResNet


传统物体检测的方法和基于深度学习的物体检测方法。

	传统方法使用 滑动窗口 的框架，把一张图分解成几百万个不同位置不同尺度的 子窗口， 针对每一个窗口使用 分类器 判断是否包含目标物体。
		传统方法针对不同的类别的物体，一般会设计不同的特征和分类算法，比如
			人脸检测的经典算法是Harr特征+Adaboosting分类器；
			行人检测的经典算法是HOG(histogram of gradients) + Support Vector Machine；
			一般性物体的检测的话是HOG的特征加上DPM(deformable part model)的算法。

	先把这三个方法过一遍，然后在进入深度学习分类中。


	物体检测RCNN系列： 其实基于深度学习的物体检测也可以看成对 海量滑动窗口 分类 ，只是用 全卷积 的方式。
		RCNN ：使用更好的CNN模型判断候选区域的类别；
		fast RCNN (Ross Girshick)：复用预计算的sharing feature map加快模型训练和物体检测的速度；
		faster RCNN ：进一步使用sharing feature map大幅提高计算候选区域的速度。


	RCNN系列算法还是将物体检测分为两个步骤。
		现在还有一些工作是端到端(end-to-end)的物体检测，比如说YOLO(You Only Look Once: Unified, Real-Time Object Detection)和SSD(SSD: Single Shot MultiBox Detector)这样的算法。这两个算法号称和faster RCNN精度相似但速度更快。
		物体检测正负样本极端非均衡，two-stage cascade可以更好的应对非均衡。端到端学习是否可以超越faster RCNN还需要更多研究实验。


	机器学习 就是学习输入到输出的一个 映射，传统方法使用浅层的 简单映射，现在深度学习是多层的 复合映射。

	
	在深度学习领域有一个简单但又非常通用的原理：
		在学习时，指导信息越丰富、越精细，学习的效果一般来说也会越好。
			
			例如1、：图像分类
				在数据量充足的情况下，图像类别的标注仅仅是动物、植物、场景的话，学习出来的模型和特征可能一般。
				但是如果把这些类别标记细化，比如最开始有十类数据，我们把它细化到一千类，例如把狗分成斑点狗、斗牛犬等，把猫分成波斯猫、大花猫等，通常来说可以学习到更好的模型和更加好的特征。
			
			总结：对图像分类时，标注越多越仔细，模型提取特征做的越好，分类更加仔细。

				此处可以指导平时工作中，对于分类，种类越多越好。

			例如2、：物体检测
				如果在bounding box的基础上增加额外的监督信息通长会得到更好的结果。
				比如标注出人脸的眼睛、鼻子、嘴的位置，人脸的角度，种族性别男女等属性，做成一个多任务学习的算法的话，通常来说能得到更好的效果。

		多标注、多任务模式：
			有时候多个标注/任务是并列关系，可以通过Multi-Task Learning的框架来学习。

			多个任务是递进关系，前一个任务的结果可以帮助后一个任务，
			例如将每一个人都独立的检测出来之后再分割每个人身体的Mask。
			合理利用这种递进关系，可以得到比并列关系更好的结果，这其实就是Instance segmentation的核心思想。

			同传统语义分割不同的是，传统语义分割只需要对物体类别进行分类，不需要区分不同的个体。
			物体分割(Instance segmentation)是既需要区分类别，又需要区分同一物体的个体，
			所以深度学习的网络需要学习到比之前语义分割任务更多的信息。


	关于：ResNet
		没有降采样的情况下，当深度达到一定的程度的时候，卷积层的学习能力是逐渐减弱的。
		当网络过深，增加的卷积层只能学习到噪音，并且造成有效信息损失，导致训练和测试loss都增加的情况。



--------------------------------------------------------------------
零基础小白，如何入门计算机视觉？
https://blog.csdn.net/electech6/article/details/79545911
--------------------------------------------------------------------

1、OpenCV是入门必看的资料：https://www.cnblogs.com/Undo-self-blog/p/8423851.html
	
	OpenCV-Python 中文教程（搬运）目录配套的pdf，仔细研究。

2、计算机视觉可以分为两大方向：
	基于学习的方法和基于几何的方法。
		
		基于学习的方法最火的就是  深度学习，
		
		基于几何方法最火的就是  视觉SLAM。
			SLAM（Simultaneous Localization and Mapping）同时定位与地图创建。
			用摄像头作为主传感器，用拍摄的视频流作为输入来实现SLAM。视觉SLAM广泛应用于VR/AR、自动驾驶、智能机器人、无人机等前沿领域。

			入门资料是 高翔（清华博士，慕尼黑理工博后）的 《视觉SLAM十四讲-从理论到实践》 。

--------------------------------------------------------------------
			计算机视觉新手入门：大佬推荐我这样学习
	https://blog.csdn.net/u013341341/article/details/79639670
--------------------------------------------------------------------

1、基础知识：

1.1 图像
	当程序在读取一张图片时，需要考虑以下数据：

	1）高度、宽度


	2）深度

		存储每个像素所用的位数，比如正常RGB的深度就是2^8*3=256*3=768，此类图片中的深度为768，每个像素点都能够代表768种颜色

	3）通道数
		
		RGB图片就是有三通道，RGBA类图片就是有四通道

	4）颜色格式

		将某种颜色表现为数字形式的模型，或者说是一种记录图像颜色的方式。
		比较常见的有：RGB模式、RGBA模式、CMYK模式、位图模式、灰度模式、索引颜色模式、双色调模式和多通道模式。


1.2 视频

	1）原始视频 = 图片序列，
		视频中的每张有序图片被称为“帧(frame)”。
		压缩后的视频，会采取各种算法减少数据的容量，其中IPB就是最常见的（传输每一帧的差别数据（IPB））。

	2）码率

		数据传输时单位时间传送的数据位数，就是取样率，单位时间取样率越大，精度就越高，即分辨率越高

	3）帧率

		每秒传输的帧数，fps全称为frames per second

	4)分辨率

		每帧图片的分辨率

	5)清晰度

		平常看片中，有不同清晰度，实际上就对应着不同的分辨率

	6)IPB

		在网络视频流中，并不是把每一帧图片全部发送到客户端来展示，而是传输每一帧的  差别数据（IPB）， 客户端然后对其进行解析，最终补充每一帧完整图片

2、深入，必须阅读原文：
	先熟悉所在方向的发展历程，历程中的里程碑式的文献必须要精读。
	
		深度学习做目标检测，RCNN、Fast RCNN、Fater RCNN、SPPNET、SSD和YOLO等模型；
		深度学习做目标跟踪，DLT、SO-DLT等等；
		对抗网络GAN、CGAN、DCGAN、LAPGAN等等。

	文献网站：https://arxiv.org/list/cs.CV/recent
		每天去更新一下别人最新的工作。

--------------------------------------------------------------------
							计算机视觉入门
		https://blog.csdn.net/gdengden/article/details/80369458
--------------------------------------------------------------------































-------------------------------------------------------------------
计算机视觉领域一些牛人博客
https://blog.csdn.net/springslx/article/details/80198187
-------------------------------------------------------------------



最难的文章：
-------------------------------------------------------------------
计算机视觉领域最全汇总
https://blog.csdn.net/eo63y6pKI42Ilxr/article/details/82322596
-------------------------------------------------------------------

