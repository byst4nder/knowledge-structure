Mask R-CNN


正文一：		五颗星  &&*****&&
----------------------------------------------------------------------------------------------
		一、实例分割模型Mask R-CNN详解：从R-CNN，Fast R-CNN，Faster R-CNN再到Mask R-CNN
				https://blog.csdn.net/jiongnima/article/details/79094159
----------------------------------------------------------------------------------------------
前言：
	何凯明：Mask R-CNN是2017年ICCV的best paper。
	传统单任务的网络结构已经逐渐不再引人瞩目，取而代之的是集成、复杂、一石多鸟的多任务网络模型：Mask R-CNN。

	1）目标检测：目标框（bounding box）
	2）目标分类：类别（class）
	3）像素级目标分割：前景，背景

	Mask R-CNN继承于Faster R-CNN（2016）,在Faster R-CNN上面加了一个Mask Prediction Branch (Mask预测分支)，改良了ROI Pooling,提出了ROI Align。

	在解析Mask R-CNN之前，先分析一下Faster R-CNN。Faster R-CNN继承于Fast R-CNN(2015)，Fast R-CNN继承于R-CNN(2014)。

	R-CNN ===> Fast R-CNN ===> Faster R-CNN ===> Mask R-CNN
	(2014)		(2015)			(2016)				(2017)


1、R-CNN:（2014年）
	Ross Girshick 采用卷积神经网络进行目标检测：
	
	综述：
		将模型输入图片===>提取2000个待检测区域===>串联方式通过卷积神经网络 提取特征===> 通过SVM分类，得到物体的类别===>
		===>通过bounding box regression调整目标包围框的大小。

	步骤：
		S1:提取待检测对象：或叫图像标注。
			模型输入为一张图片，然后在图片上提取2000多个待检测区域。
				selective search:
					通过传统图像处理方法将图像分成若干块，然后通过一个SVM将属于同一目标的若干块拿出来。
					核心是一个SVM。分类。
		
		S2:特征提取：
			借助当时2012年最新成果AlexNet，网络模型：通过使用图像分类数据集训练一个用于提取特征的网络。

		S3:比对目标：
			使用SVM,用目标的标签（类别）与包围框的大小进行训练。


	贡献：
		1）使用了卷积神经网络进行特征提取
		2）使用了bounding box regression 进行目标包围框的修正。

	问题：
		1）耗时的selective search,一帧图像花费2秒
		2）耗时的串行式CNN前向传播，对于每一个ROI，都需要经过AlexNet提取特征，为所有的ROI提取特征。
		3）*** 三个模块分别训练 ***，对存储空间的消耗很大。

2、Fast R-CNN（2015年）
	为解决如上问题：
		Ross在2015年，提出Fast R-CNN：

		===> 还是采用selective search 提取2000个候选框;
		===> 通过一个神经网络对全图进行特征提取;
		===> 使用ROI Pooling Layer在全图上摘取每一个ROI对应的特征;
		===> 再通过全连接层（FC Layer）进行分类和包围框的修正。

	贡献：
		1）取代R-CNN的串行特征提取方式，
		2）除了selective search ,其他部分都可以结合在一起训练。

	缺点：耗时的selective search 依旧存在。

3、Faster R-CNN（2016年）
	为了改良selective search的耗时问题：
		取代Selective Search,直接通过一个Region Proposal network(RPN)生成待检测区域，
		在生成ROI区域的时候，时间也就从2s缩减到10ms。

	过程：
		S1：===>>> 共享卷积层为全图提取特征feature maps
		S2：===>>> 将features maps送入到RPN,
		S3：===>>> RPN生成待检测框（指定ROI的位置），并对ROI的包围框进行第一次修正。
		S4：===>>> 进入到Fast R-CNN架构，
					ROI Pooling Layer根据RPN的输出在feature map上面选择每个ROI对应的特征，并将维度值置为定值。
		S5：===>>> 全连接层（FC Layer）对框进行分类，并且对目标包围框进行第二次修正。 

	S1,S2,S3替换掉Selective Search和Fast R-CNN中的特征提取。

	Faster R-CNN 真正实现了端到端的训练（end-to-end training）。

	要理解Mask R-CNN，只有先理解Faster R-CNN。

	Faster R-CNN架构：
		三大部分：
			1) 共享的卷积层-backbone
			2) 候选区域生成网络-RPN
			3) 对候选区域进行分类的网络-classifier
			其中：RPN和classifier部分均对目标框有修正。classifier完全继承Fast R-CNN结构。

		模块细节：
			RPN工作原理：
				RPN依靠一个在共享特征图 conv feature map 上滑动的窗口 sliding window. 

				为每个位置生成9种预先设置好长宽比与面积的目标框（anchor）,
					三种面积（128*128， 256*256， 512*512）
					三种长宽比（1：1， 1:2， 2:1）

				共享特征图feature maps大约有 40 * 60， RPN生成的初始anchor总数为20000个左右（40* 60 *9）。

				RPN目的：
					1）判断anchor到底是前景还是背景：判断这个anchor到底有没有覆盖目标。
						使用SoftmaxLoss直接训练，在训练的时候排除掉超越边界的anchor;
					2）为属于前景的anchor进行第一次坐标修正。
						采用SmoothL1Loss进行训练。

				RPN的实现：
					RPN本质：树状结构。
						树干为 3x3 的卷积层，树枝为两个1x1的卷积层。
							第一个1×1的卷积层解决了 ***前后景的输出，***
							第二个1×1的卷积层解决了 ***边框修正的输出。***

				RPN细节：
					对于RPN输出的特征图中的每一个点，
					第一个1×1的卷积层输出了18个值：前后景的输出：
						因为每一个点对应9个anchor,每个anchor有一个前景分数和一个后景分数（9×2）。
							前景后景区分：
								如果一个anchor与ground truth的IoU在0.7以上，这个anchor就算前景positive;
								如果这个anchor与ground truth的IoU在0.3以下，这个anchor就算背景(negative)。

							作者进行RPN网络训练的时候，没有使用IoU在0.3和0.7之间的anchor,
							在训练anchor属于前景与背景的时候，是在一张图中，随机抽取了128个前景anchor与128个背景anchor。

								补充：（IoU：Intersection over Union :IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。）
				
					第二个1×1的卷积层输出了36个值：边框修正。
						因为是每一个点对应9个anchor，每个anchor对应了4个修正坐标的值9×4=36。
						边框修正主要由4个值完成，tx,ty,th,tw。
							tx和ty：修正后的框在anchor的x和y方向上做出的平移。
							th和tw：修正前的框长宽各自放大一定的倍数。
						如何训练：
							采用SmothL1Loss进行训练。
							不是对于所有的anchor，都需要进行anchor包围框修正的参数训练，只是对positive的anchors有这一步。
							因此，在训练RPN的时候，只有对128个随机抽取的positive anchors有这一步训练。


			RoI Pooling:
				为何需要RoI Pooling?
					在Fast R-CNN中，特征被共享卷积层一次性提取。
					对于每个RoI而言，需要从 共享卷积层上 **摘取** 对应的特征,并且送入全连接层进行分类。
					RoI Pooling主要做了两件事：
						第一件是为每个RoI选取对应的特征，
						第二件是为满足全连接层的输入需求。
							将每个RoI对应的特征的维度转化为某个定值。
						
						对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小


			Fast R-CNN的分类器：
				提取的RoI具体是什么类别(人，车，马等等)，一共C+1类(包含一类背景)。


			RoI边框修正训练
				RoI边框修正和RPN中的anchor边框修正原理一样，同样也是SmoothL1 Loss。
				RoI边框修正也是对于非背景的RoI进行修正，对于类别标签为背景的RoI，则不进行RoI边框修正的参数训练。

			   在训练分类器和RoI边框修正时，步骤如下所示：

				   0) 首先通过共享卷积层backbone提取特征,shared conv feature maps (40×60)。

				   1) 首先通过RPN生成约20000个anchor(40×60×9)。

				   2) 对20000个anchor进行第一次边框修正，得到修订边框后的proposal。

				   3) 对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围。

				   4) 忽略掉长或者宽太小的proposal。

				   5) 将所有proposal按照前景分数从高到低排序，选取前12000个proposal。

				   6) 使用阈值为0.7的NMS算法排除掉重叠的proposal。

				   7) 首先通过RPN生成约20000个anchor(40×60×9)。

		总的来说，Faster R-CNN的loss分两大块，第一大块是训练RPN的loss(包含一个SoftmaxLoss和SmoothL1Loss)，第二大块是训练Fast R-CNN中分类器的loss(包含一个SoftmaxLoss和SmoothL1Loss)，

		Faster R-CNN的训练方式有三种，描述如下：	
		   1) RPN和Fast R-CNN交替训练，这种方式也是作者采用的方式。

		   2) 近似联合RPN和Fast R-CNN的训练，在训练时忽略掉了RoI边框修正的误差，只对anchor做了边框修订，"近似联合"。

		   3) 联合RPN和Fast R-CNN的训练。

		交替训练的方式，步骤如下：

		   1) 使用在ImageNet上预训练的模型初始化共享卷积层并训练RPN。

		   2) 使用上一步得到的RPN参数生成RoI proposal。
				再使用ImageNet上预训练的模型初始化共享卷积层，训练Fast R-CNN部分(分类器和RoI边框修订)。

		   3) 将训练后的共享卷积层参数固定，同时将Fast R-CNN的参数固定，训练RPN。
				(从这一步开始，共享卷积层的参数真正被两大块网络共享)

		   4) 同样将共享卷积层参数固定，并将RPN的参数固定，训练Fast R-CNN部分。


		Faster R-CNN的测试流程和训练流程挺相似，描述如下：

		   1) 首先通过RPN生成约20000个anchor(40×60×9)通过RPN。

		   2) 对20000个anchor进行第一次边框修正，得到修订边框后的proposal。

		   3) 对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围。

		   4) 忽略掉长或者宽太小的proposal。

		   5) 将所有proposal按照前景分数从高到低排序，选取前6000个proposal。

		   6) 使用阈值为0.7的NMS算法排除掉重叠的proposal。

		   7) 针对上一步剩下的proposal,选取前300个proposal进行分类和第二次边框修正。


4、Mask R-CNN (2017年)
	Mask R-CNN只是在Faster R-CNN上面加了一个Mask Prediction Branch (Mask 预测分支)，并且改良了ROI Pooling，提出了ROI Align。

	RoI Pooling的问题：

		问题1：从输入图上的RoI到特征图上的RoI feature，RoI Pooling是直接通过四舍五入 取整 得到的结果。
			直接用round取的值，RoI Pooling过后的得到的输出可能和原图像上的RoI对不上。
			RoI Pooling Layer的四舍五入取整操作，可能会导致其进行了偏移。

		问题2：在将每个RoI对应的特征转化为固定大小的维度时，又采用了 取整 操作。

			这种取整操作(在Mask R-CNN中被称为quantization)对RoI分类影响不大，
			可是对逐像素的预测目标是有害的，因为对每个RoI取得的特征并没有与RoI对齐。
			因此，Mask R-CNN对RoI Pooling做了改进并提出了RoI Align。

			   RoI Align的主要创新点：
				   针对问题1，不再进行取整操作。
				   针对问题2，使用双线性插值来更精确地找到每个块对应的特征。
			   总的来说，
				   RoI Align的作用主要就是剔除了RoI Pooling的取整操作，
				   并且使得为每个RoI取得的特征能够更好地对齐原图上的RoI区域。

		在Mask R-CNN中的RoI Align之后有一个"head"部分，主要作用是将RoI Align的输出维度扩大，这样在预测Mask时会更加精确。
		
		在Mask Branch的训练环节，没有采用FCN式的SoftmaxLoss，反而是输出了K个Mask预测图(为每一个类都输出一张)，
		并采用average binary cross-entropy loss训练。
		
		当然在训练Mask branch的时候，输出的K个特征图中，也只是对应ground truth类别的那一个特征图对Mask loss有贡献。
			Lbox和Lmask都是对positive RoI才会起作用的。


	Mask R-CNN和FCIS做个比较：
		相同点：均继承了Faster R-CNN的RPN部分。
		不同点：
			对于FCIS，预测mask和分类是共享的参数。
			而Mask R-CNN则是各玩各的，两个任务各自有各自的可训练参数。




补充一：				
--------------------------------------------------------------------------------------------
		二、深度学习中IU、IoU(Intersection over Union)的概念理解以及python程序实现
		https://blog.csdn.net/iamoldpan/article/details/78799857
		https://oldpan.me/archives/understand-coco-metric				
--------------------------------------------------------------------------------------------
	IoU(Intersection over Union)：一种测量在特定数据集中检测相应物体准确度的一个标准。
		只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用IoU来进行测量。

		1）ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）；
		2）我们的算法得出的结果范围。
	这个标准用于测量真实和预测之间的相关度，相关度越高，该值越高。

	IoU相当于两个区域重叠的部分除以两个区域的集合部分得出的结果。
	一般来说，这个score ＞ 0.5 就可以被认为一个不错的结果了。






补充二：
----------------------------------------------------------------------------
			三、目标检测之Loss：FasterRCNN中的SmoothL1Loss
		https://blog.csdn.net/wfei101/article/details/79809332
----------------------------------------------------------------------------
多任务损失： 多任务！！！！
	Fast R-CNN网络有两个同级输出层（cls score和bbox_prdict层），都是全连接层，称为multi-task。






补充三：
--------------------------------------------------------------------------------------------
				四、NMS和soft-nms算法
		https://www.cnblogs.com/zf-blog/p/8532228.html
--------------------------------------------------------------------------------------------

NMS在物体检测中的应用：

	物体检测中应用NMS算法的主要目的是消除多余（交叉重复）的窗口，找到最佳物体检测位置。






补充四、
--------------------------------------------------------------------------------------------
		五、实例分割初探，Fully Convolutional Instance-aware Semantic Segmentation论文解读
				https://blog.csdn.net/jiongnima/article/details/78961147
--------------------------------------------------------------------------------------------
	本文重新建立一个文档。
	深度学习计算机视觉领域：
		图像分类(AlexNet, VGG16等等)
		图像分割(以FCN为代表的一众论文)
		目标检测(R-CNN，Fast R-CNN和Fatser R-CNN，以及后来的YOLO和SSD，目标检测领域已经实现多任务)。

	   1. 将物体从背景中分离(测试结果上只是没有画出目标框)，即 目标检测。

	   2. 对检测到的物体进行逐像素提取，即 图像分割。

	   3. 对检测到的物体进行类别划分，即 图像分类。

  实例分割是一个很综合的问题，融合了目标检测，图像分割与图像分类。







正文二：  	五颗星  &&*****&&
----------------------------------------------------------------------------
							六、Mask R-CNN详解
		https://blog.csdn.net/WZZ18191171661/article/details/79453780
----------------------------------------------------------------------------
亮点：
	1、对目标检测、语义分割以及实例分割，进行了说明。
	2、博采众长，集大成者。




一、Mask R-CNN是什么，可以做哪些任务？

	Mask R-CNN 是一个 实例分割（Instance segmentation）算法，可以用来做“目标检测”、 “目标实例分割”、 “目标关键点检测”。

	1、 实例分割（Instance segmentation）和语义分割（Semantic segmentation）的区别与联系。
	
	联系：语义分割和实例分割都是目标分割中的两个小的领域，都是用来对输入的图片做分割处理；
	区别：图示化的方法区别：
		对象检测：框选对象图像区域																		：框选级别：
		语义分割：对于同类属性图像区域轮廓提取分割。														：区域色块：
		实例分割：对象检测的区域中框中，选择出相同语义的部分，对语义分割中的轮廓，分割成一个个的实例。		：个体像素级别：


	即 【实例分割】 需要在 【语义分割】 的基础上对 【同类物体】 进行 【更精细】 的 分割。


	2、 Mask R-CNN可以完成的任务：
		Mask R-CNN是一个非常灵活的框架，可以增加不同的分支完成不同的任务，可以完成目标分类、目标检测、语义分割、实例分割、人体姿势识别等多种任务！

	3、 目标与实现：
		高速和高准确率：
			选用了经典的 目标检测算法 Faster-rcnn 和经典的 语义分割算法 FCN。 

				Faster-rcnn可以既快又准的完成目标检测的功能；
				FCN可以精准的完成语义分割的功能，这两个算法都是对应领域中的经典之作。

			Mask R-CNN比Faster-rcnn复杂，但是最终仍然可以达到5fps的速度，这和原始的Faster-rcnn的速度相当。

			由于发现了ROI Pooling中所存在的像素偏差问题，提出了对应的ROIAlign策略，加上FCN精准的像素MASK，使得其可以获得高准确率。

		简单直观：
			整个 【Mask R-CNN】 算法的思路很简单，就是在 【原始Faster-rcnn】 算法的基础上面 【增加了FCN】 来产生对应的 【MASK分支】 。
			即Faster-rcnn + FCN，更细致的是 【RPN + ROIAlign + Fast-rcnn + FCN】。

		易于使用：
			整个 【Mask R-CNN】 算法非常的灵活，可以用来完成多种任务，包括 【目标分类、目标检测、语义分割、实例分割、人体姿态识别】 等多个任务，这将其易于使用的特点展现的淋漓尽致。很少见到有哪个算法有这么好的扩展性和易用性，值得我们学习和借鉴。
			除此之外，我们可以更换不同的 backbone architecture 和 Head Architecture 来获得不同性能的结果。


二、Mask R-CNN框架解析：
	
	1、 Mask R-CNN算法步骤：
		·首先，输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；
		·然后，将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；
		·接着，对这个feature map中的每一点设定预定个的ROI，从而获得多个候选ROI；
		·接着，将这些候选的ROI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的ROI；
		·接着，对这些剩下的ROI进行ROIAlign操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）；
		·最后，对这些ROI进行分类（N类别分类）、BB回归和MASK生成（在每一个ROI里面进行FCN操作）。


	2、 在这里将Mask R-CNN分解为如下的3个模块，Faster-rcnn、ROIAlign和FCN。然后分别对这3个模块进行讲解，这也是该算法的核心。



	3、 Faster-rcnn：
		----------------------------------------------------------
		Faster-rcnn详解
		https://blog.csdn.net/WZZ18191171661/article/details/
		----------------------------------------------------------
		Faster-RCNN算法精读
		https://blog.csdn.net/hunterlew/article/details/71075925
		----------------------------------------------------------
	4、 FCN：
		FCN算法是一个经典的语义分割算法，可以对图片中的目标进行准确的分割。
		其总体架构类似 U 型，它是一个端到端的网络，
			
			主要的模快包括卷积和去卷积，即先对图像进行卷积和池化，使其feature map的大小不断减小；
			
			然后进行反卷积操作，即进行插值操作，不断的增大其feature map，

			最后对每一个像素值进行分类。从而实现对输入图像的准确分割。


	5、 ROIPooling和ROIAlign的分析与比较：
		
		前者使用了两次量化操作，而后者并没有采用量化操作，使用了线性插值算法。
		此处解释详细：仔细阅读：

















----------------------------------------------------------------------------
				七、Mask RCNN 源代码解析 (1) - 整体思路
		https://blog.csdn.net/hnshahao/article/details/81231211
----------------------------------------------------------------------------









----------------------------------------------------------------------------
				八、(Mask RCNN)——论文详解(真的很详细)
		https://blog.csdn.net/wangdongwei0/article/details/83110305
----------------------------------------------------------------------------







----------------------------------------------------------------------------
							九、Mask RCNN笔记
		https://blog.csdn.net/xiamentingtao/article/details/78598511
----------------------------------------------------------------------------







----------------------------------------------------------------------------
	十、Mask R-CNN+tensorflow/keras的配置介绍、代码详解与训练自己的数据集演示
		https://blog.csdn.net/Xiongchao99/article/details/79106588
----------------------------------------------------------------------------





----------------------------------------------------------------------------
		十一、Mask_RCNN翻译和详解笔记一（原文翻译+源代码+代码使用说明）
	https://blog.csdn.net/weixin_40355324/article/details/81390724
----------------------------------------------------------------------------

名词表

	Mask R-CNN（Mask Recycle Convolutional Neural Network），掩膜循环卷积神经网络

	LBP（Local Binary Pattern，局部二值模式）
		是一种用来描述图像局部纹理特征的算子；
		具有旋转不变性和灰度不变性等显著的优点。
		用于纹理特征提取，提取的特征是图像的局部的纹理特征。

	HOG（Histogram of Oriented Gradient,），
		方向梯度直方图特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。
		它通过计算和统计图像局部区域的梯度方向直方图来构成特征。
		Hog特征结合SVM分类器已经被广泛应用于图像识别中，尤其在行人检测中获得了极大的成功。
		HOG+SVM进行行人检测的方法是2005的CVPR上提出的，如今很多行人检测算法不断提出，但基本都是以HOG+SVM的思路为主。

	SSD(Single Shot MultiBox Detector),单镜头多盒(多目标)检测器

	Haar-like特征最早应用于人脸表示，在此基础上，使用3种类型4种形式的特征。
		Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。
		特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。
		Haar特征值反映了图像的灰度变化情况。
		例如：脸部的一些特征能由矩形特征简单的描述，
			如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。
		但矩形特征只对一些简单的图形结构，如边缘、线段较敏感，所以只能描述特定走向（水平、垂直、对角）的结构。


	imgaug安装：
	git clone https://github.com/aleju/imgaug
	cd imgaug 
	python setup.py install


	pycocotools安装：
	https://github.com/philferriere/cocoapi
	git clone https://github.com/philferriere/cocoapi.git
	cd PythonAPI
	python3 setup.py install 



	Step by Step Detection:
	检测步骤:
	S1：1. Anchor sorting and filtering
	S2：2. Bounding Box Refinement
	S3：3. Mask Generation
	S4：4.Layer activations
	S5：5. Weight Histograms
	S6：6. Logging to TensorBoard
	S7：7. Composing the different pieces into a final result





----------------------------------------------------------------------------
					论文阅读学习 - Mask R-CNN
		https://blog.csdn.net/zziahgf/article/details/78730859
----------------------------------------------------------------------------



----------------------------------------------------------------------------

----------------------------------------------------------------------------



----------------------------------------------------------------------------


----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------


----------------------------------------------------------------------------

----------------------------------------------------------------------------